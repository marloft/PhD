{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'pima-indians-diabetes.data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c86b09322a57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'pima-indians-diabetes.data.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadCsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loaded data file {0} with {1} rows'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-c86b09322a57>\u001b[0m in \u001b[0;36mloadCsv\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloadCsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'pima-indians-diabetes.data.csv'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Dec  6 22:45:16 2015\n",
    "@author: maryl\n",
    "NAIVE BAYES CLASSIFIER\n",
    "HANDLE DATA\n",
    "Load Data File\n",
    "Split into Training & Test\n",
    "SUMMARISE DATA\n",
    "Separate Data By Class\n",
    "Calculate Mean\n",
    "Calculate Standard Deviation\n",
    "Summarize Dataset\n",
    "Summarize Attributes By Class\n",
    "MAKE PREDICTION\n",
    "Calculate Gaussian Probability Density Function\n",
    "Calculate Class Probabilities\n",
    "Make a Prediction\n",
    "Estimate Accuracy\n",
    "Calculate Gaussian Probability Density Function\n",
    "MAKE PREDICTIONS\n",
    "GET ACCURACY\n",
    "Handle Data\n",
    "The first thing we need to do is load our data file. The data is in CSV format without a header line or any quotes. We can open the file with the open function and read the data lines using the reader function in the csv module.\n",
    "We also need to convert the attributes that were loaded as strings into numbers that we can work with them. Below is the loadCsv() function for loading the Pima indians dataset.\n",
    "http://machinelearningmastery.com/naive-bayes-classifier-scratch-python/\n",
    "\"\"\"\n",
    "import csv\n",
    "def loadCsv(filename):\n",
    "\tlines = csv.reader(open(filename, \"rb\"))\n",
    "\tdataset = list(lines)\n",
    "\tfor i in range(len(dataset)):\n",
    "\t\tdataset[i] = [float(x) for x in dataset[i]]\n",
    "\treturn dataset\n",
    " \n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "dataset = loadCsv(filename)\n",
    "print('Loaded data file {0} with {1} rows').format(filename, len(dataset))\n",
    "\n",
    "\"\"\"\n",
    "Next we need to split the data into a training dataset that Naive Bayes can use to make predictions and a test dataset that we can use to evaluate the accuracy of the model. We need to split the data set randomly into train and datasets with a ratio of 67% train and 33% test (this is a common ratio for testing an algorithm on a dataset).\n",
    "Below is the splitDataset() function that will split a given dataset into a given split ratio.\"\"\"\n",
    "\n",
    "import random\n",
    "def splitDataset(dataset, splitRatio):\n",
    "\ttrainSize = int(len(dataset) * splitRatio)\n",
    "\ttrainSet = []\n",
    "\tcopy = list(dataset)\n",
    "\twhile len(trainSet) < trainSize:\n",
    "\t\tindex = random.randrange(len(copy))\n",
    "\t\ttrainSet.append(copy.pop(index))\n",
    "\treturn [trainSet, copy]\n",
    " \n",
    "dataset = [[1], [2], [3], [4], [5]]\n",
    "splitRatio = 0.67\n",
    "train, test = splitDataset(dataset, splitRatio)\n",
    "print('Split {0} rows into train with {1} and test with {2}').format(len(dataset), train, test)\n",
    "\n",
    "\"\"\"Separate Data By Class\n",
    "The first task is to separate the training dataset instances by class value so that we can calculate statistics for each class. We can do that by creating a map of each class value to a list of instances that belong to that class and sort the entire dataset of instances into the appropriate lists.\n",
    "\"\"\"\n",
    "def separateByClass(dataset):\n",
    "\tseparated = {}\n",
    "\tfor i in range(len(dataset)):\n",
    "\t\tvector = dataset[i]\n",
    "\t\tif (vector[-1] not in separated):\n",
    "\t\t\tseparated[vector[-1]] = []\n",
    "\t\tseparated[vector[-1]].append(vector)\n",
    "\treturn separated\n",
    " \n",
    "dataset = [[1,20,1], [2,21,0], [3,22,1]]\n",
    "separated = separateByClass(dataset)\n",
    "print('Separated instances: {0}').format(separated)\n",
    "\n",
    "\"\"\"Calculate Mean\n",
    "We need to calculate the mean of each attribute for a class value. The mean is the central middle or central tendency of the data, and we will use it as the middle of our gaussian distribution when calculating probabilities.\n",
    "We also need to calculate the standard deviation of each attribute for a class value. The standard deviation describes the variation of spread of the data, and we will use it to characterize the expected spread of each attribute in our Gaussian distribution when calculating probabilities.\n",
    "The standard deviation is calculated as the square root of the variance. The variance is calculated as the average of the squared differences for each attribute value from the mean. Note we are using the N-1 method, which subtracts 1 from the number of attribute values when calculating the variance.\n",
    "\"\"\"\n",
    "import math\n",
    "def mean(numbers):\n",
    "\treturn sum(numbers)/float(len(numbers))\n",
    "\n",
    "def stdev(numbers):\n",
    "\tavg = mean(numbers)\n",
    "\tvariance = sum([pow(x-avg,2) for x in numbers])/float(len(numbers)-1)\n",
    "\treturn math.sqrt(variance)\n",
    " \n",
    "#We can test this by taking the mean of the numbers from 1 to 5.\n",
    "numbers = [1,2,3,4,5]\n",
    "print('Summary of {0}: mean={1}, stdev={2}').format(numbers, mean(numbers), stdev(numbers))\n",
    "\n",
    "\"\"\"\n",
    "Summarize Dataset\n",
    "Now we have the tools to summarize a dataset. For a given list of instances (for a class value) we can calculate the mean and the standard deviation for each attribute.\n",
    "The zip function groups the values for each attribute across our data instances into their own lists so that we can compute the mean and standard deviation values for the attribute.\"\"\"\n",
    "def summarize(dataset):\n",
    "\tsummaries = [(mean(attribute), stdev(attribute)) for attribute in zip(*dataset)]\n",
    "\tdel summaries[-1]\n",
    "\treturn summaries\n",
    " \n",
    "#Test\n",
    "dataset = [[1,20,0], [2,21,1], [3,22,0]]\n",
    "summary = summarize(dataset)\n",
    "print('Attribute summaries: {0}').format(summary)\n",
    "\n",
    "\"\"\"Summarize Attributes By Class\n",
    "We can pull it all together by first separating our training dataset into instances grouped by class. Then calculate the summaries for each attribute. \"\"\"\n",
    "def summarizeByClass(dataset):\n",
    "\tseparated = separateByClass(dataset)\n",
    "\tsummaries = {}\n",
    "\tfor classValue, instances in separated.iteritems():\n",
    "\t\tsummaries[classValue] = summarize(instances)\n",
    "\treturn summaries\n",
    " \n",
    "#Test\n",
    "dataset = [[1,20,1], [2,21,0], [3,22,1], [4,22,0]]\n",
    "summary = summarizeByClass(dataset)\n",
    "print('Summary by class value: {0}').format(summary)\n",
    "\n",
    "\"\"\"3. Make Prediction\n",
    "We are now ready to make predictions using the summaries prepared from our training data. Making predictions involves calculating the probability that a given data instance belongs to each class, then selecting the class with the largest probability as the prediction.\n",
    "We can divide this part into the following tasks:\n",
    "Calculate Gaussian Probability Density Function\n",
    "Calculate Class Probabilities\n",
    "Make a Prediction\n",
    "Estimate Accuracy\n",
    "Calculate Gaussian Probability Density Function\n",
    "We can use a Gaussian function to estimate the probability of a given attribute value, given the known mean and standard deviation for the attribute estimated from the training data.\n",
    "Given that the attribute summaries where prepared for each attribute and class value, the result is the conditional probability of a given attribute value given a class value.\n",
    "See the references for the details of this equation for the Gaussian probability density function. In summary we are plugging our known details into the Gaussian (attribute value, mean and standard deviation) and reading off the likelihood that our attribute value belongs to the class.\n",
    "In the calculateProbability() function we calculate the exponent first, then calculate the main division. This lets us fit the equation nicely on two lines. \"\"\"\n",
    "import math\n",
    "def calculateProbability(x, mean, stdev):\n",
    "\texponent = math.exp(-(math.pow(x-mean,2)/(2*math.pow(stdev,2))))\n",
    "\treturn (1 / (math.sqrt(2*math.pi) * stdev)) * exponent\n",
    "#Test \n",
    "x = 71.5\n",
    "mean = 73\n",
    "stdev = 6.2\n",
    "probability = calculateProbability(x, mean, stdev)\n",
    "print('Probability of belonging to this class: {0}').format(probability)\n",
    "\n",
    "\"\"\"\n",
    "Calculate Class Probabilities\n",
    "Now that we can calculate the probability of an attribute belonging to a class, we can combine the probabilities of all of the attribute values for a data instance and come up with a probability of the entire data instance belonging to the class.\n",
    "We combine probabilities together by multiplying them. In the calculateClassProbabilities() below, the probability of a given data instance is calculated by multiplying together the attribute probabilities for each class. the result is a map of class values to probabilities.\"\"\"\n",
    "def calculateClassProbabilities(summaries, inputVector):\n",
    "\tprobabilities = {}\n",
    "\tfor classValue, classSummaries in summaries.iteritems():\n",
    "\t\tprobabilities[classValue] = 1\n",
    "\t\tfor i in range(len(classSummaries)):\n",
    "\t\t\tmean, stdev = classSummaries[i]\n",
    "\t\t\tx = inputVector[i]\n",
    "\t\t\tprobabilities[classValue] *= calculateProbability(x, mean, stdev)\n",
    "\treturn probabilities\n",
    " \n",
    " #Test\n",
    "summaries = {0:[(1, 0.5)], 1:[(20, 5.0)]}\n",
    "inputVector = [1.1, '?']\n",
    "probabilities = calculateClassProbabilities(summaries, inputVector)\n",
    "print('Probabilities for each class: {0}').format(probabilities)\n",
    "\n",
    "\"\"\"Make a Prediction\n",
    "Now that can calculate the probability of a data instance belonging to each class value, we can look for the largest probability and return the associated class.\"\"\"\n",
    "def predict(summaries, inputVector):\n",
    "\tprobabilities = calculateClassProbabilities(summaries, inputVector)\n",
    "\tbestLabel, bestProb = None, -1\n",
    "\tfor classValue, probability in probabilities.iteritems():\n",
    "\t\tif bestLabel is None or probability > bestProb:\n",
    "\t\t\tbestProb = probability\n",
    "\t\t\tbestLabel = classValue\n",
    "\treturn bestLabel\n",
    "\n",
    "#Test\n",
    "summaries = {'A':[(1, 0.5)], 'B':[(20, 5.0)]}\n",
    "inputVector = [1.1, '?']\n",
    "result = predict(summaries, inputVector)\n",
    "print('Prediction: {0}').format(result)\n",
    "\n",
    "\"\"\"4. Make Predictions\n",
    "Finally, we can estimate the accuracy of the model by making predictions for each data instance in our test dataset. The getPredictions() will do this and return a list of predictions for each test instance.\n",
    "\"\"\"\n",
    "def getPredictions(summaries, testSet):\n",
    "\tpredictions = []\n",
    "\tfor i in range(len(testSet)):\n",
    "\t\tresult = predict(summaries, testSet[i])\n",
    "\t\tpredictions.append(result)\n",
    "\treturn predictions\n",
    " \n",
    "#Test\n",
    "summaries = {'A':[(1, 0.5)], 'B':[(20, 5.0)]}\n",
    "testSet = [[1.1, '?'], [19.1, '?']]\n",
    "predictions = getPredictions(summaries, testSet)\n",
    "print('Predictions: {0}').format(predictions)\n",
    "\n",
    "\"\"\"5. Get Accuracy\n",
    "The predictions can be compared to the class values in the test dataset and a classification accuracy can be calculated as an accuracy ratio between 0& and 100%. The getAccuracy() will calculate this accuracy ratio.\n",
    "\"\"\" \n",
    "def getAccuracy(testSet, predictions):\n",
    "\tcorrect = 0\n",
    "\tfor x in range(len(testSet)):\n",
    "\t\tif testSet[x][-1] == predictions[x]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn (correct/float(len(testSet))) * 100.0\n",
    " \n",
    " #Test\n",
    "testSet = [[1,1,1,'a'], [2,2,2,'a'], [3,3,3,'b']]\n",
    "predictions = ['a', 'a', 'a']\n",
    "accuracy = getAccuracy(testSet, predictions)\n",
    "print('Accuracy: {0}').format(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
